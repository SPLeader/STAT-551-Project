---
title: "AQI_ModelTuning_py"
author: "Sean Leader // William Medwid // Vanessa Veto // Jose Mierzejewski"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: visual
execute:
  message: false
---

```{python}
import pandas as pd
import numpy as np

from sklearn.dummy import DummyRegressor
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedKFold
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor

import seaborn as sns
import matplotlib.pyplot as plt
import os
```

```{python}
import warnings
warnings.filterwarnings('ignore')
```


# Data Preparation

```{python}
os.chdir('../')
print(os.getcwd())

aqi_ozone = pd.read_csv("Project Data/Combined/aqi_ozone_all_years_prediction.csv")

aqi_ozone = aqi_ozone.drop(["Date", "State", "County", "AQI_cat", 'AQI', 'max_ozone', 'mean_ozone', 'min_ozone'], axis = 1)
```

```{python}
X = aqi_ozone.drop(columns=['AQI_next_day'])
y = aqi_ozone['AQI_next_day']
all_features = list(X.columns)
```

```{python}
# Creating k fold cv's
cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=13)
```

```{python}
# Training and validation sets for single trained models (not cross-val)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10)
```




```{python}
# Getting the SSE so that we can calculate r2 values later
dummy_regr = DummyRegressor(strategy="mean")

scores = cross_val_score(dummy_regr, X, y, cv = cv, scoring = 'neg_mean_squared_error')
SSE = -scores.sum()/len(scores)

# SSE value confirmed to be the same as with GridSearchCV
SSE
```

```{python}
model_scores = pd.DataFrame(columns = ["Model", "R-Squared", "Root Mean Squared Error"])
```


## Linear models

### Tuning basic linear model

```{python}
#Tuning different mixture models

scaler = StandardScaler()
model = ElasticNet()

pipe = Pipeline(steps=[("scaler", scaler), ("model", model)])

# define grid
grid = dict()
grid['model__alpha'] = [1e-10, 1e-5, 1.0]
grid['model__l1_ratio'] = [0, 0.5, 1.0] #np.arange(0, 1, 0.5)
# define search
search = GridSearchCV(pipe, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)
# perform the search
results = search.fit(X, y)

# summarize
rmse = -results.best_score_
r2 = ((-results.best_score_)**2) / SSE

# scores not recorded because polynomial is better.
#model_scores.loc[len(model_scores)] = ["Linear Model", r2, rmse]

print('RMSE: %.3f' % )
print('Config: %s' % results.best_params_)
```

**Best model: second lowest alpha, ridge**

### Tuning polynomial linear model

```{python}
#Tuning different mixture models with poly degree = 2

poly = PolynomialFeatures(2)
scaler = StandardScaler()
model = ElasticNet()

pipe = Pipeline(steps=[("poly", poly), ("scaler", scaler), ("model", model)])

# define grid
grid = dict()
grid['model__alpha'] = [1e-10, 1e-5, 1.0]
grid['model__l1_ratio'] = [0, 0.5, 1.0] #np.arange(0, 1, 0.5)
# define search
search = GridSearchCV(pipe, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)
# perform the search
results = search.fit(X, y)


# summarize
rmse = -results.best_score_
r2 = ((-results.best_score_)**2) / SSE
model_scores.loc[len(model_scores)] = ["Linear Model", r2, rmse]

print('RMSE: %.3f' % rmse)
print('Config: %s' % results.best_params_)
```

**Best model: Polynomial 2, alpha 1e-05, ridge**

### Single linear model residuals

```{python}
# Fitting a mixture model to plot residuals

poly = PolynomialFeatures(2)
scaler = StandardScaler()
model = ElasticNet(alpha = 1e-5, l1_ratio = 0)


pipe = Pipeline(steps=[("poly", poly), ("scaler", scaler), ("model", model)])
# define model evaluation method

# fit the model
fitted_model = pipe.fit(X_train, y_train)

lm_preds = fitted_model.predict(X_test)

df_lm_preds = pd.DataFrame(data = {
  'preds' : lm_preds, 
  'residuals' : lm_preds - y_test,
  'model' : "Linear Model"
  })
```


```{python}
sns.lmplot(x='preds',
    y='residuals',  
    data = df_lm_preds, lowess = True)
    
plt.show()
plt.close()
```

### Interpretable linear model coef

```{python}
# Fitting a mixture model to plot residuals

scaler = StandardScaler()
model = ElasticNet(alpha = 3.5, l1_ratio = 1)


pipe = Pipeline(steps=[("scaler", scaler), ("model", model)])
# define model evaluation method

search = GridSearchCV(pipe, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)
# perform the search
fitted_model = pipe.fit(X_train, y_train)


fitted_model[1].coef_
coef_df = pd.DataFrame(
  data = {"coef" : fitted_model[1].coef_,
  "abs_coef" : abs(fitted_model[1].coef_)},
  index = X.columns
)

print("Intercept: %f" %fitted_model[1].intercept_)

coef_df.sort_values(by = "abs_coef", ascending = False)
```

```{python}
# Finding the best performance we can get with just the best 3 predictors.
X_subset = X[["max_no2", "max_temp", "max_humid"]]

#Tuning different mixture models

scaler = StandardScaler()
model = ElasticNet()


pipe = Pipeline(steps=[("scaler", scaler), ("model", model)])

# Define model evaluation method
cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=13)

# Define grid
grid = dict()
grid['model__alpha'] = [1e-10, 1e-5, 1.0]
grid['model__l1_ratio'] = [0, 0.5, 1.0]
# define search
search = GridSearchCV(pipe, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)

# Perform the search
results = search.fit(X_subset, y)

# Summarize best model
print('RMSE: %.3f' % -results.best_score_)
print('Config: %s' % results.best_params_)
```

The best RMSE with just 3 best variables was 26.818, not too much worse than the best models using all variables.

# Random Forest

### Tuning

```{python}
# Tuning Random Forest
model = RandomForestRegressor(n_estimators = 100, random_state = 13)

# Define model evaluation method
cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=13)

grid = dict()
grid['min_samples_leaf'] = [1, 2]
grid['max_features'] = [4, 6, 8]

search = GridSearchCV(model, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)
# Perform the search
results = search.fit(X, y)

# summarize
rmse = -results.best_score_
r2 = ((-results.best_score_)**2) / SSE
model_scores.loc[len(model_scores)] = ["Random Forest", r2, rmse]

print('RMSE: %.3f' % rmse)
print('Config: %s' % results.best_params_)
```

### Single Random Forest Restiduals

```{python}
# fit the model
best_rf_model = RandomForestRegressor(min_samples_leaf = 1, max_features = 8, n_estimators = 100, random_state = 13)

fitted_model = best_rf_model.fit(X_train, y_train)

rf_preds = fitted_model.predict(X_test)

df_rf_preds = pd.DataFrame(data = {
  'preds' : rf_preds, 
  'residuals' : rf_preds - y_test,
  'model' : "Random Forest"
  })
```


```{python}
sns.lmplot(x='preds',
    y='residuals',  
    data = df_rf_preds, lowess = True)
    
plt.show()
plt.close()
```

Compared to our linear model, a random forest is much less biased. For its predictions between 0 and 100,the random forest isn't perfect, but isn't biased high or low. Above 100 predicted AQI, the random forest develops a tendency to under-predict, however.

# Decision Tree

### Tuning

```{python}
# Tuning Decision Tree
model = DecisionTreeRegressor(random_state = 13)

# define model evaluation method
cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=13)

grid = dict()
grid['max_depth'] = [2, 4, 6, 8, 10]
grid['min_impurity_decrease'] = [1, 2, 5]

search = GridSearchCV(model, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=1)
# perform the search
results = search.fit(X, y)

# summarize
rmse = -results.best_score_
r2 = ((-results.best_score_)**2) / SSE
model_scores.loc[len(model_scores)] = ["Decision Tree", r2, rmse]

print('RMSE: %.3f' % rmse)
print('Config: %s' % results.best_params_)
```

### Single Decision Tree for residuals

```{python}
tree = DecisionTreeRegressor(max_depth = 6, 
  min_impurity_decrease = 2)

tree_fit = tree.fit(X_train, y_train)

dt_preds = tree_fit.predict(X_test)

df_dt_preds = pd.DataFrame(data = {
  'preds' : dt_preds, 
  'residuals' : dt_preds - y_test,
  'model' : "Decision Tree"
  })
```

```{python}
sns.lmplot(x='preds',
    y='residuals',  
    data = df_dt_preds, lowess = True)
    
plt.show()
plt.close()
```

### Single Random Forest to plot

```{python}
tree = DecisionTreeRegressor(max_depth=2)

tree_fit = tree.fit(X_train, y_train)


plot_tree(tree_fit, 
  proportion=True, 
  max_depth=2, 
  feature_names=X.columns,
  filled=True)
        
    
plt.show()


os.chdir('../')

plt.savefig("AQI Outputs/AQI_decision_tree_py.png")
plt.close()
```

# Combined Results Plot

```{python}
# Output predictions as CSVs
os.chdir('../')
#all_preds = df_lm_preds.append(df_rf_preds).append(df_dt_preds)

df_lm_preds.to_csv("AQI Outputs/AQI_df_lm_preds_py.csv")
df_dt_preds.to_csv("AQI Outputs/AQI_df_dt_preds_py.csv")
df_rf_preds.to_csv("AQI Outputs/AQI_df_rf_preds_py.csv")
```

```{python}
sns.set_theme()
plt.figure().set_figwidth(20)
plt.figure().set_figheight(1)

fig, axs = plt.subplots(3, 1, sharex=True, sharey = True)
axs[0].scatter(df_lm_preds["preds"], df_lm_preds["residuals"], s=.3)
axs[0].axhline(y = 0, color = 'k', linestyle = '--', linewidth=.5)
axs[0].set_title('LM', fontdict={'fontsize' : 8})
axs[1].scatter(df_dt_preds["preds"], df_dt_preds["residuals"], s=.3)
axs[1].axhline(y = 0, color = 'k', linestyle = '--', linewidth=.5)
axs[1].set_title('DT', fontdict={'fontsize' : 8})
axs[2].scatter(df_rf_preds["preds"], df_rf_preds["residuals"], s=.3)
axs[2].axhline(y = 0, color = 'k', linestyle = '--', linewidth=.5)
axs[2].set_title('RF', fontdict={'fontsize' : 8})


plt.show()
plt.close()
```

```{python}
# Outputting model metrics
os.chdir('../')

model_scores.to_csv("AQI Outputs/AQI_model_metrics_py.csv")
```

